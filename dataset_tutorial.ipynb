{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dataset_tutorial.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "_e1UlEVlREoC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Tensorflow Dataset API\n",
        "\n",
        "在Tensorflow中，feed-dict方式对Model传输数据速率慢， 使用管道输入(pipeline)传输速率比较快，而Tensorflow内置了一个API(tf.data)，它可以方便地处理数据以及高效地传输数据给Model。\n",
        "这篇我将会讲述tf.data API基本机制和一些常用的操作。\n",
        "\n",
        "tf.data API主要有两个对象：tf.data.Dataset和tf.data.Iterator  \n",
        "tf.data.Dataset：存储一列表数据元素的Tensor对象。  \n",
        "tf.data.Iterator： 提供访问Dataset的数据元素的主要方式  \n",
        "\n",
        "### 大概提纲\n",
        "* 创建数据集(Create Dataset)\n",
        "* 创建迭代器(Craete Iterator)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "cBqqsCJROZ2k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 创建数据集(Craete Dataset)\n",
        "Dataset用于存储我们的数据元素"
      ]
    },
    {
      "metadata": {
        "id": "EtCXJL41P02e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KlSdTHdVOdz9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "1. From Numpy"
      ]
    },
    {
      "metadata": {
        "id": "V5hkksDFQBNh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f2ee5d2f-7c12-4457-db09-7a16c5fece3f"
      },
      "cell_type": "code",
      "source": [
        "# 创建随机数据\n",
        "x = np.random.sample((5,2))\n",
        "# 创建Dataset\n",
        "dataset = tf.data.Dataset.from_tensor_slices(x)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<DatasetV1Adapter shapes: (2,), types: tf.float64>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "abHTiZ_tPKvS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fc5414c6-9b14-43ef-8b88-3b5e8e7a55d8"
      },
      "cell_type": "code",
      "source": [
        "# 创建两个numpy array\n",
        "x, y = np.random.sample((5,2)), np.random.sample((5,1))\n",
        "# 两个numpy array同时创建Dataset\n",
        "dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "\n",
        "dataset"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<DatasetV1Adapter shapes: ((2,), (1,)), types: (tf.float64, tf.float64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "YgamjDBAP1p_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "2. From Tensor对象"
      ]
    },
    {
      "metadata": {
        "id": "UtOjEV7SP6bS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ad3947b8-e3d2-47ca-8337-9720ca8ac3ec"
      },
      "cell_type": "code",
      "source": [
        "a = tf.random_uniform([5,2])\n",
        "dataset = tf.data.Dataset.from_tensor_slices(a)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<DatasetV1Adapter shapes: (2,), types: tf.float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "WCHFFsU0QI32",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "3. From Placeholder"
      ]
    },
    {
      "metadata": {
        "id": "28aRFIWiQISk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "631ffb57-c360-42de-fda1-7c9abbfe7644"
      },
      "cell_type": "code",
      "source": [
        "# 通过placeholder来创建Dataset，可以动态改变数据的来源，在训练Model的时候非常有用，比如训练数据集和测试数据集\n",
        "input = tf.placeholder(tf.float64, shape=[None, 2])\n",
        "dataset = tf.data.Dataset.from_tensor_slices(input)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<DatasetV1Adapter shapes: (2,), types: tf.float64>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "ZRL9IjDRRDk1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "4. From CSV文件"
      ]
    },
    {
      "metadata": {
        "id": "oYx0ZK1gRGPK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "650ae760-9547-4584-892f-cbaeaaba12ea"
      },
      "cell_type": "code",
      "source": [
        "# 通过CSV文件数据创建Dataset\n",
        "csv_file = 'GSPC.csv'\n",
        "dataset = tf.contrib.data.make_csv_dataset(csv_file, batch_size=32)\n",
        "\n",
        "iterator = dataset.make_one_shot_iterator()\n",
        "next_element = iterator.get_next()\n",
        "\n",
        "Date, Open = next_element['Date'], next_element['Open']\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  print(sess.run([Date, Open]))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-8-9addea93784e>:2: make_csv_dataset (from tensorflow.contrib.data.python.ops.readers) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.make_csv_dataset(...)`.\n",
            "[array([b'2019-02-07', b'2019-02-19', b'2019-02-14', b'2019-02-27',\n",
            "       b'2019-02-20', b'2019-03-05', b'2019-03-06', b'2019-02-25',\n",
            "       b'2019-02-15', b'2019-02-26', b'2019-03-04', b'2019-02-21',\n",
            "       b'2019-02-08', b'2019-02-12', b'2019-03-01', b'2019-02-13',\n",
            "       b'2019-02-28', b'2019-02-11', b'2019-02-22', b'2019-02-13',\n",
            "       b'2019-03-04', b'2019-02-20', b'2019-02-25', b'2019-02-12',\n",
            "       b'2019-02-22', b'2019-02-21', b'2019-03-01', b'2019-02-07',\n",
            "       b'2019-02-15', b'2019-02-14', b'2019-02-26', b'2019-02-28'],\n",
            "      dtype=object), array([2717.53, 2769.28, 2743.5 , 2787.5 , 2779.05, 2794.41, 2790.27,\n",
            "       2804.35, 2760.24, 2792.36, 2814.37, 2780.24, 2692.36, 2722.61,\n",
            "       2798.22, 2750.3 , 2788.11, 2712.4 , 2780.67, 2750.3 , 2814.37,\n",
            "       2779.05, 2804.35, 2722.61, 2780.67, 2780.24, 2798.22, 2717.53,\n",
            "       2760.24, 2743.5 , 2792.36, 2788.11], dtype=float32)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "a-N9eY0YU9o3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 创建迭代器(Create Iterator)\n",
        "Iterator用于访问和获取Dataset的每一个数据元素"
      ]
    },
    {
      "metadata": {
        "id": "SIWoiZcjbvft",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "1. One-hot Iterator  \n",
        "one-hot是最简单的迭代器(Iterator)，它可以处理大多数的数据管道输入(pipline)的情况。"
      ]
    },
    {
      "metadata": {
        "id": "qhmADkwGVDGM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "89bad5f8-3d90-4efa-de21-59acc61b5542"
      },
      "cell_type": "code",
      "source": [
        "# 创建Dataset\n",
        "x = np.random.sample((5,2))\n",
        "dataset = tf.data.Dataset.from_tensor_slices(x)\n",
        "\n",
        "# 创建Iterator\n",
        "iterator = dataset.make_one_shot_iterator()\n",
        "next_element = iterator.get_next()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  for _ in range(5):\n",
        "    # 获取每个元素\n",
        "    print(sess.run(next_element))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.85679913 0.10143722]\n",
            "[0.88175387 0.55230242]\n",
            "[0.00634388 0.84738638]\n",
            "[0.75764302 0.8207592 ]\n",
            "[0.60181308 0.59532544]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0qe_GGOPdFym",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "2. Initializable Iterator  \n",
        "它可以初始化不同数据的，但还是同一个Dataset， 比如训练数据集和测试数据集。  \n",
        "它配合placeholder来使用"
      ]
    },
    {
      "metadata": {
        "id": "t4lbFE0vdDm3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "02c6ad0f-86ee-43ca-8f71-060ebf1ab9c7"
      },
      "cell_type": "code",
      "source": [
        "# 创建Dataset\n",
        "input = tf.placeholder(tf.float64,shape=[None, 2])\n",
        "dataset = tf.data.Dataset.from_tensor_slices(input)\n",
        "\n",
        "train_x = np.random.sample((5,2))\n",
        "test_x = np.random.sample((3,2))\n",
        "\n",
        "# 创建Iterator\n",
        "iterator = dataset.make_initializable_iterator()\n",
        "next_element = iterator.get_next()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  # trainning model\n",
        "  # 使用trainning数据集，并初始化Iteraotr\n",
        "  sess.run(iterator.initializer, feed_dict={input: train_x})\n",
        "  for _ in range(5):\n",
        "    print(sess.run(next_element))\n",
        "    \n",
        "  print(\"--------\")\n",
        "  \n",
        "  # evaluate model\n",
        "  # 使用测试数据集，初始化Iterator\n",
        "  sess.run(iterator.initializer, feed_dict={input: test_x})\n",
        "  for _ in range(3):\n",
        "    print(sess.run(next_element))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.58777345 0.34376131]\n",
            "[0.07595064 0.3270094 ]\n",
            "[0.47187587 0.13764359]\n",
            "[0.93592063 0.983476  ]\n",
            "[0.21084482 0.88479164]\n",
            "--------\n",
            "[0.59344643 0.7610382 ]\n",
            "[0.75931599 0.21857005]\n",
            "[0.69312244 0.24552298]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zQR8T545h5Oc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "2. Reinitializable Iterator  \n",
        "reinitializable可以初始化两个不同数据和不同Dataset"
      ]
    },
    {
      "metadata": {
        "id": "oJh2rKjIidja",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "05f07aac-d17a-443a-8af0-ea335566b3e4"
      },
      "cell_type": "code",
      "source": [
        "# 创建Dataset\n",
        "training_dataset = tf.data.Dataset.from_tensor_slices(np.random.sample((5,2)))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices(np.random.sample((3,2)))\n",
        "\n",
        "# 创建Iterator\n",
        "iterator = tf.data.Iterator.from_structure(training_dataset.output_types,\n",
        "                                          training_dataset.output_shapes)\n",
        "next_element = iterator.get_next()\n",
        "\n",
        "# 创建两个不同数据集的初始化\n",
        "training_init_op = iterator.make_initializer(training_dataset)\n",
        "test_init_op = iterator.make_initializer(test_dataset)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(training_init_op)\n",
        "  for _ in range(5):\n",
        "    print(sess.run(next_element))\n",
        "   \n",
        "  print(\"---------\")\n",
        "  \n",
        "  sess.run(test_init_op)\n",
        "  for _ in range(3):\n",
        "    print(sess.run(next_element))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.45166403 0.74806626]\n",
            "[0.75083313 0.80168765]\n",
            "[0.34276602 0.71042542]\n",
            "[0.45131295 0.32137104]\n",
            "[0.92210205 0.04995708]\n",
            "---------\n",
            "[0.18816433 0.22128761]\n",
            "[0.32184117 0.61118122]\n",
            "[0.62841448 0.75044657]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MDDIi4ndnva5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "3. Feedable Iterator  \n",
        "feedable类似与reinitialazable，但是feedable是创建两个不同的Iterator，来自不同数据集，切换Iterator时不用重新初始化"
      ]
    },
    {
      "metadata": {
        "id": "7gvNd4wKWGw9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "fa7e3307-0808-4d60-e829-496571e945ee"
      },
      "cell_type": "code",
      "source": [
        "training_dataset = tf.data.Dataset.from_tensor_slices(np.random.sample((5,2)))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices(np.random.sample((3,2)))\n",
        "\n",
        "handle = tf.placeholder(tf.string, shape=[])\n",
        "iterator = tf.data.Iterator.from_string_handle(handle,\n",
        "                                               training_dataset.output_types,\n",
        "                                              training_dataset.output_shapes)\n",
        "next_element = iterator.get_next()\n",
        "\n",
        "training_iterator = training_dataset.make_one_shot_iterator()\n",
        "test_iterator = test_dataset.make_initializable_iterator()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  training_handle = sess.run(training_iterator.string_handle())\n",
        "  test_handle = sess.run(test_iterator.string_handle())\n",
        "  \n",
        "  for _ in range(5):\n",
        "    print(sess.run(next_element, feed_dict={handle: training_handle}))\n",
        "  \n",
        "  print(\"----------\")\n",
        "  \n",
        "  sess.run(test_iterator.initializer)\n",
        "  for _ in range(3):\n",
        "    print(sess.run(next_element, feed_dict={handle: test_handle}))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.7223556  0.94651226]\n",
            "[0.77856537 0.51863231]\n",
            "[0.91456479 0.18817305]\n",
            "[0.60656066 0.78488042]\n",
            "[0.50343049 0.98391472]\n",
            "----------\n",
            "[0.9246092  0.39914318]\n",
            "[0.8977477  0.16831537]\n",
            "[0.76376902 0.30904837]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6MpxpWUOG-8A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 例子\n",
        "前面的例子都是通过在Session中打印get_next()的值，下面通过一个例子，来实现Dataset的数据传值给Model来训练。"
      ]
    },
    {
      "metadata": {
        "id": "7DMGPmCSQdav",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### One-hot的例子"
      ]
    },
    {
      "metadata": {
        "id": "J8PK6JMSIL3c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "c2522682-ea56-412f-f36e-4c0b0becaf28"
      },
      "cell_type": "code",
      "source": [
        "# 生成数据\n",
        "features = np.random.sample((100, 1))\n",
        "labels = 2 * features + 1.5\n",
        "\n",
        "epoches = 10\n",
        "batch_size = 32\n",
        "\n",
        "# 创建Dataset\n",
        "dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
        "dataset = dataset.batch(batch_size).repeat()\n",
        "\n",
        "iterator = dataset.make_one_shot_iterator()\n",
        "x, y = iterator.get_next()\n",
        "\n",
        "# 建立Model\n",
        "layer1 = tf.layers.dense(x, 4, activation=tf.tanh)\n",
        "layer2 = tf.layers.dense(layer1, 4, activation=tf.tanh)\n",
        "predictions = tf.layers.dense(layer2,1, activation=tf.sigmoid)\n",
        "\n",
        "loss = tf.losses.mean_squared_error(predictions, y)\n",
        "optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  for i in range(epoches):\n",
        "    _, l = sess.run([optimizer, loss])\n",
        "    print(\"Epoch: {}, Loss: {}\".format((i+1), l))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, Loss: 4.268806457519531\n",
            "Epoch: 2, Loss: 3.5479159355163574\n",
            "Epoch: 3, Loss: 3.6658082008361816\n",
            "Epoch: 4, Loss: 4.615617752075195\n",
            "Epoch: 5, Loss: 4.222705841064453\n",
            "Epoch: 6, Loss: 3.5081288814544678\n",
            "Epoch: 7, Loss: 3.623868703842163\n",
            "Epoch: 8, Loss: 4.564029693603516\n",
            "Epoch: 9, Loss: 4.177005767822266\n",
            "Epoch: 10, Loss: 3.4687089920043945\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sSwKB-KHQihE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Initializable例子"
      ]
    },
    {
      "metadata": {
        "id": "m6MIJX8kQwzP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "f84b3c3f-e651-43c6-c062-14c3c98233f9"
      },
      "cell_type": "code",
      "source": [
        "# 生成数据\n",
        "training_data = (np.random.sample((100,2)), np.random.sample((100, 1)))\n",
        "test_data = (np.random.sample((30, 2)), np.random.sample((30,1)))\n",
        "\n",
        "epoches = 10\n",
        "batch_size = 32\n",
        "\n",
        "input, labels = tf.placeholder(tf.float32, shape=[None, 2]), tf.placeholder(tf.float32, shape=[None,1])\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input, labels)).batch(batch_size).repeat()\n",
        "\n",
        "\n",
        "iterator = dataset.make_initializable_iterator()\n",
        "x, y = iterator.get_next()\n",
        "\n",
        "# 建立Model\n",
        "layer1 = tf.layers.dense(x, 4, activation=tf.tanh)\n",
        "layer2 = tf.layers.dense(layer1, 4, activation=tf.tanh)\n",
        "predictions = tf.layers.dense(layer2,1, activation=tf.sigmoid)\n",
        "\n",
        "loss = tf.losses.mean_squared_error(predictions, y)\n",
        "optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  sess.run(iterator.initializer, feed_dict={input: training_data[0], \n",
        "                                           labels: training_data[1]})\n",
        "  print(\"Training...\")\n",
        "  for i in range(epoches):\n",
        "    total_loss = 0\n",
        "    for _ in range(10):\n",
        "      _, l = sess.run([optimizer, loss])\n",
        "      total_loss += l\n",
        "    print(\"Epoches: {}, Loss: {}\".format(i, total_loss / 10))\n",
        "  \n",
        "  print(\"Testing...\")\n",
        "  sess.run(iterator.initializer, feed_dict={input: test_data[0],\n",
        "                                           labels: test_data[1]})\n",
        "  print(\"Test loss: {}\".format(sess.run(loss)))\n",
        "    "
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training...\n",
            "Epoches: 0, Loss: 0.10809506624937057\n",
            "Epoches: 1, Loss: 0.11347251757979393\n",
            "Epoches: 2, Loss: 0.10589925572276115\n",
            "Epoches: 3, Loss: 0.11198231056332589\n",
            "Epoches: 4, Loss: 0.10426002144813537\n",
            "Epoches: 5, Loss: 0.11087124645709992\n",
            "Epoches: 6, Loss: 0.10295571163296699\n",
            "Epoches: 7, Loss: 0.11003697961568833\n",
            "Epoches: 8, Loss: 0.10191557928919792\n",
            "Epoches: 9, Loss: 0.10939777716994285\n",
            "Testing...\n",
            "Test loss: 0.08818259835243225\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "s1WWIQHeX9m1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Datset API为我们提供了快捷和高效的方式来生成数据集输入管道(pipeline)，可为Model快速训练，评估，测试。"
      ]
    }
  ]
}